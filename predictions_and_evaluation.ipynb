{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ IMPORTS ============\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "import quantstats as qs\n",
    "\n",
    "# =========== CONFIGURATION ============\n",
    "target_col = \"Label_7day\"\n",
    "# ======================================\n",
    "\n",
    "# ============ LOAD LABELLED DATA ============\n",
    "predictions_df = pd.read_csv(\"data_cache/labelled_df.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# ============ SPLIT DATA ============\n",
    "split_idx = int(len(predictions_df) * 0.8)\n",
    "predictions_df[\"Set\"] = \"Train\"\n",
    "predictions_df.loc[predictions_df.index[split_idx:], \"Set\"] = \"Test\"\n",
    "\n",
    "# Define X, y\n",
    "drop_cols = [target_col, \"Close\",\"Barrier_Hit_Day\",\"Near_Peak\",\"Actual_Return_7day\",\"Set\"]  # keep only features\n",
    "feature_cols = [col for col in predictions_df.columns if col not in drop_cols]\n",
    "\n",
    "X_train = predictions_df.loc[predictions_df[\"Set\"] == \"Train\", feature_cols]\n",
    "y_train = predictions_df.loc[predictions_df[\"Set\"] == \"Train\", target_col]\n",
    "X_test = predictions_df.loc[predictions_df[\"Set\"] == \"Test\", feature_cols]\n",
    "y_test = predictions_df.loc[predictions_df[\"Set\"] == \"Test\", target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== MLFLOW SETUP ============\n",
    "experiment  = mlflow.get_experiment_by_name(\"triple_barrier_classification\")\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "# =========== LIST PREVIOUS RUNS ============\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "runs = runs.sort_values(\"start_time\", ascending=False)\n",
    "runs = runs[runs[\"status\"] == \"FINISHED\"]  # Filter for finished runs\n",
    "\n",
    "# ========== LOAD LATEST MODELS ==========\n",
    "model_names = ['log_reg', 'random_forest', 'svm_rbf']\n",
    "latest_run_df = runs.groupby('tags.mlflow.runName').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== PREDICTIONS ============\n",
    "for model_name in model_names:\n",
    "    model = mlflow.sklearn.load_model(f\"runs:/{latest_run_df.loc[model_name, 'run_id']}/model\")\n",
    "    predictions_df[f'pred_{model_name}'] = model.predict(predictions_df[feature_cols])  # Predict on full dataset\n",
    "    # Store per-row probability arrays as lists so they can be stacked later\n",
    "    predictions_df[f'proba_{model_name}'] = list(model.predict_proba(predictions_df[feature_cols]))  # Get probabilities\n",
    "\n",
    "# predictions_df.to_csv('data_cache/predictions_df.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== EVALUATION ============\n",
    "results = []\n",
    "# for model_name in model_names:\n",
    "model_name = \"random_forest\"  # Just evaluate the random forest\n",
    "y_pred = predictions_df.loc[predictions_df[\"Set\"] == \"Test\", f'pred_{model_name}']\n",
    "# Stack per-row probability arrays into a (n_samples, n_classes) array\n",
    "y_proba = predictions_df.loc[predictions_df[\"Set\"] == \"Test\", f'proba_{model_name}'].values\n",
    "y_proba = np.vstack(list(y_proba))\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "auc = roc_auc_score(y_test, y_proba, average=\"weighted\", multi_class=\"ovo\")\n",
    "\n",
    "results.append({\n",
    "    \"Model\": model_name,\n",
    "    \"Accuracy\": acc,\n",
    "    \"F1 Score\": f1,\n",
    "    \"ROC AUC\": auc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ============ TRADING METRICS ============\n",
    "\n",
    "# Get actual returns for train and test set\n",
    "actual_returns_train = predictions_df.loc[X_train.index, \"Actual_Return_7day\"]\n",
    "actual_returns_test = predictions_df.loc[X_test.index, \"Actual_Return_7day\"]\n",
    "\n",
    "# Filter to only traded signals (where model predicted -1 or 1)\n",
    "traded_mask = (y_pred != 0)\n",
    "traded_returns = actual_returns_test[traded_mask]\n",
    "traded_pred = y_pred[traded_mask]\n",
    "traded_true = y_test[traded_mask]\n",
    "\n",
    "# Convert to decimal returns (QuantStats expects decimals, not %)\n",
    "returns_decimal = traded_returns / 100\n",
    "\n",
    "# Calculate trading metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRADING PERFORMANCE METRICS (QuantStats)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Win Rate:              {qs.stats.win_rate(returns_decimal):.1%}\")\n",
    "print(f\"Profit Factor:         {qs.stats.profit_factor(returns_decimal):.2f}\")\n",
    "print(f\"Avg Win:               {qs.stats.avg_win(returns_decimal):.2%}\")\n",
    "print(f\"Avg Loss:              {qs.stats.avg_loss(returns_decimal):.2%}\")\n",
    "print(f\"Max Drawdown:          {qs.stats.max_drawdown(returns_decimal):.2%}\")\n",
    "print(f\"Sharpe Ratio:          {qs.stats.sharpe(returns_decimal):.2f}\")\n",
    "\n",
    "# Custom metric: Avg Loss When Wrong (supervisor requirement)\n",
    "wrong_predictions_mask = (traded_pred != np.sign(traded_true))\n",
    "wrong_returns = traded_returns[wrong_predictions_mask]\n",
    "avg_loss_when_wrong = abs(wrong_returns[wrong_returns < 0].mean())\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"AVG LOSS WHEN WRONG:   {avg_loss_when_wrong:.2%}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Calculate Expected Value manually (QuantStats doesn't have direct EV)\n",
    "win_rate = qs.stats.win_rate(returns_decimal)\n",
    "avg_win = qs.stats.avg_win(returns_decimal)\n",
    "avg_loss = abs(qs.stats.avg_loss(returns_decimal))\n",
    "expected_value = (win_rate * avg_win) - ((1 - win_rate) * avg_loss)\n",
    "\n",
    "# Optional: Generate HTML report for dissertation appendix\n",
    "returns_series = pd.Series(returns_decimal.values, index=X_test.index[traded_mask])\n",
    "qs.reports.html(returns_series, output='results/model_performance_tearsheet.html', \n",
    "                title=f'{name} Trading Performance')\n",
    "print(f\"\\nGenerated HTML report: results/model_performance_tearsheet.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ES327Py3.13.11",
   "language": "python",
   "name": "es327py31311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
