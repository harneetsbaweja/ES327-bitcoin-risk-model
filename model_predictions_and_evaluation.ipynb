{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ IMPORTS ============\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "import quantstats as qs\n",
    "from src.calculate_ml_metrics import calculate_ml_metrics\n",
    "from src.calculate_trade_metrics import calculate_trade_metrics\n",
    "\n",
    "# =========== CONFIGURATION ============\n",
    "target_col = \"Label_7day\"\n",
    "# ======================================\n",
    "\n",
    "# ============ LOAD LABELLED DATA ============\n",
    "# ============ LOAD LABELLED DATA ============\n",
    "labelled_data_cache = pd.read_pickle('data_cache/labelled_data.pkl')\n",
    "predictions_df = labelled_data_cache['labelled_df']\n",
    "X_train = labelled_data_cache['X_train']\n",
    "X_test = labelled_data_cache['X_test']\n",
    "y_train = labelled_data_cache['y_train']\n",
    "y_test = labelled_data_cache['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== MLFLOW SETUP ============\n",
    "experiment  = mlflow.get_experiment_by_name(\"triple_barrier_classification\")\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "# =========== LIST PREVIOUS RUNS ============\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "runs = runs.sort_values(\"start_time\", ascending=False)\n",
    "runs = runs[runs[\"status\"] == \"FINISHED\"]  # Filter for finished runs\n",
    "\n",
    "# ========== LOAD LATEST MODELS ==========\n",
    "model_names = ['log_reg', 'random_forest', 'svm_rbf']\n",
    "latest_run_df = runs.groupby('tags.mlflow.runName').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== PREDICTIONS ============\n",
    "for model_name in model_names:\n",
    "    model = mlflow.sklearn.load_model(f\"runs:/{latest_run_df.loc[model_name, 'run_id']}/model\")\n",
    "    predictions_df[f'pred_{model_name}'] = model.predict(predictions_df[feature_cols])  # Predict on full dataset\n",
    "    # Store per-row probability arrays as lists so they can be stacked later\n",
    "    predictions_df[f'proba_{model_name}'] = list(model.predict_proba(predictions_df[feature_cols]))  # Get probabilities\n",
    "\n",
    "# predictions_df.to_csv('data_cache/predictions_df.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== EVALUATION ============\n",
    "ml_metrics = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    train_pred = predictions_df.loc[predictions_df[\"Set\"] == \"Train\", f'pred_{model_name}']\n",
    "    train_proba = np.vstack(predictions_df.loc[predictions_df[\"Set\"] == \"Train\", f'proba_{model_name}'])\n",
    "    test_pred = predictions_df.loc[predictions_df[\"Set\"] == \"Test\", f'pred_{model_name}']\n",
    "    test_proba = np.vstack(predictions_df.loc[predictions_df[\"Set\"] == \"Test\", f'proba_{model_name}'])\n",
    "    \n",
    "    train_ml_metrics_dict = calculate_ml_metrics(y_train, train_pred, train_proba, \"train\")\n",
    "    test_ml_metrics_dict = calculate_ml_metrics(y_test, test_pred, test_proba, \"test\")\n",
    "    ml_metrics_dict = {\"Model\": model_name, **train_ml_metrics_dict, **test_ml_metrics_dict}\n",
    "    ml_metrics.append(ml_metrics_dict)\n",
    "\n",
    "# ============ STORE RESULTS ============\n",
    "ml_metrics = pd.DataFrame(ml_metrics)\n",
    "ml_metrics.to_csv('data_cache/ml_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TRADING METRICS ============\n",
    "trade_metrics = []\n",
    "model_trade_metrics_dict = {}\n",
    "\n",
    "# Get actual returns for train and test set\n",
    "actual_returns_train = predictions_df.loc[X_train.index, \"Actual_Return_7day\"]\n",
    "actual_returns_test = predictions_df.loc[X_test.index, \"Actual_Return_7day\"]\n",
    "\n",
    "# Filter to only traded signals (where model predicted -1 or 1)\n",
    "for model_name in model_names:\n",
    "    train_pred = predictions_df.loc[predictions_df[\"Set\"] == \"Train\", f'pred_{model_name}']\n",
    "    train_proba = np.vstack(predictions_df.loc[predictions_df[\"Set\"] == \"Train\", f'proba_{model_name}'])\n",
    "    test_pred = predictions_df.loc[predictions_df[\"Set\"] == \"Test\", f'pred_{model_name}']\n",
    "    test_proba = np.vstack(predictions_df.loc[predictions_df[\"Set\"] == \"Test\", f'proba_{model_name}'])\n",
    "    \n",
    "    train_trade_metrics_dict = calculate_trade_metrics(y_train, train_pred, actual_returns_train, \"train\")\n",
    "    test_trade_metrics_dict = calculate_trade_metrics(y_test, test_pred, actual_returns_test, \"test\")\n",
    "    trade_metrics_dict = {\"Model\": model_name, **train_trade_metrics_dict, **test_trade_metrics_dict}\n",
    "    trade_metrics.append(trade_metrics_dict)\n",
    "    # Store returns for potential further analysis or visualization\n",
    "    \n",
    "    test_traded_mask = (test_pred != 0)\n",
    "    test_traded_returns = actual_returns_test[test_traded_mask] / 100\n",
    "    test_traded_pred = test_pred[test_traded_mask]\n",
    "    test_traded_true = y_test[test_traded_mask]\n",
    "    test_wrong_predictions = (test_traded_pred != np.sign(test_traded_true))\n",
    "    test_wrong_returns = test_traded_returns[test_wrong_predictions]\n",
    "    test_avg_loss_when_wrong = abs(test_wrong_returns[test_wrong_returns < 0].mean())\n",
    "    \n",
    "    # Same calculations for train set\n",
    "    train_traded_mask = (train_pred != 0)\n",
    "    train_traded_returns = actual_returns_train[train_traded_mask] / 100\n",
    "    train_traded_pred = train_pred[train_traded_mask]\n",
    "    train_traded_true = y_train[train_traded_mask]\n",
    "    train_wrong_predictions = (train_traded_pred != np.sign(train_traded_true))\n",
    "    train_wrong_returns = train_traded_returns[train_wrong_predictions]\n",
    "    train_avg_loss_when_wrong = abs(train_wrong_returns[train_wrong_returns < 0].mean())\n",
    "    \n",
    "    model_trade_metrics_dict[model_name] = {\n",
    "        'train_traded_mask': train_traded_mask,\n",
    "        'train_traded_returns': train_traded_returns,\n",
    "        'train_traded_pred': train_traded_pred,\n",
    "        'train_traded_true': train_traded_true,\n",
    "        \"train_returns\": train_traded_returns,\n",
    "        \"train_wrong_predictions\": train_wrong_predictions,\n",
    "        \"train_wrong_returns\": train_wrong_returns,\n",
    "        'train_avg_loss_when_wrong': train_avg_loss_when_wrong,\n",
    "        'test_traded_mask': test_traded_mask,\n",
    "        'test_traded_returns': test_traded_returns,\n",
    "        'test_traded_pred': test_traded_pred,\n",
    "        'test_traded_true': test_traded_true,\n",
    "        \"test_returns\": test_traded_returns,\n",
    "        \"test_wrong_predictions\": test_wrong_predictions,\n",
    "        \"test_wrong_returns\": test_wrong_returns,\n",
    "        'test_avg_loss_when_wrong': test_avg_loss_when_wrong  \n",
    "    }\n",
    "    \n",
    "    #  Dataframe of model trade metrics\n",
    "    globals()[f'model_trade_metrics_{model_name}'] = pd.DataFrame(model_trade_metrics_dict[model_name])\n",
    "        \n",
    "# # ============ SHOW & STORE RESULTS ============\n",
    "# trade_metrics = pd.DataFrame(trade_metrics)\n",
    "# print(trade_metrics.to_markdown(index=False))\n",
    "# trade_metrics.to_csv('data_cache/trade_metrics.csv', index=False)\n",
    "\n",
    "# HTML Report with QuantStats\n",
    "# returns_series = pd.Series(returns_decimal.values, index=X_test.index[traded_mask])\n",
    "# qs.reports.html(returns_series, output='data_cache/model_performance_tearsheet.html', \n",
    "#                 title=f'{model_name} Trading Performance')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ES327Py3.13.11",
   "language": "python",
   "name": "es327py31311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
